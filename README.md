# ğŸ“„ Resume Parser Backend (LLM + Rule-Based Hybrid)

This project is a modular backend system to parse resumes (PDF/DOCX), extract structured information like name, email, skills, education, experience, and more using both **LLM-based** and **rule-based** methods.

---

## âœ… Features

* ğŸ“¤ Upload support for PDF and DOCX resumes
* ğŸ§  Dual Parsing Modes: LLM-based (GPT-4o) and Rule-based
* ğŸ” Accurate field extraction (name, email, skills, etc.)
* ğŸªª OCR fallback for scanned PDFs
* ğŸ§± Clean modular architecture for scalability
* ğŸ“Š Optional Streamlit UI for visualizing parsed data

---

## ğŸ—‚ï¸ Folder Structure

```
resume_parser/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ models/             # Pydantic models
â”‚   â”‚   â””â”€â”€ resume_models.py
â”‚   â””â”€â”€ services/           # Core logic modules
â”‚       â”œâ”€â”€ llm_parser.py         # GPT-4o based parsing
â”‚       â”œâ”€â”€ rule_based_parser.py  # Regex-based parsing fallback
â”‚       â”œâ”€â”€ text_extractor.py     # PDF/DOCX + OCR extractors
â”‚       â””â”€â”€ config.py             # Env configs (API keys etc.)
â”‚   â””â”€â”€ main.py             # FastAPI entrypoint
â”œâ”€â”€ streamlit_app.py        # Optional Streamlit UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                    # API keys and config
â”œâ”€â”€ .gitignore
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/resume-parser-backend.git
cd resume-parser-backend
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install OCR Support (Linux)

```bash
sudo apt update
sudo apt install tesseract-ocr poppler-utils
```

### 4. Setup `.env`

Create a `.env` file in the root directory with:

```
OPENAI_API_KEY=your_openai_api_key
```

### 5. Run the API Server

```bash
uvicorn app.main:app --reload
```

Server will start at: `http://localhost:8000`

---

## ğŸ“¤ API Usage

### `POST /parse-resume/`

Upload a resume file:

**Request (form-data):**

* `file`: PDF or DOCX resume
* `mode`: `"llm"` or `"rule"` (default is `"llm"`)

**Curl Example:**

```bash
curl -X POST http://localhost:8000/parse-resume/ \
  -F "file=@resume.pdf" \
  -F "mode=llm"
```

**Response:**

```json
{
  "parsed_resume": {
    "name": "John Doe",
    "email": "john@example.com",
    "phone": "+1-234-567-890",
    "skills": ["Python", "ML"],
    ...
  }
}
```

---

## ğŸ›ï¸ Streamlit UI (Optional)

Run the visualization UI:

```bash
streamlit run streamlit_app.py
```

---

## ğŸ§  Parsing Modes

| Mode   | Description                                   |
| ------ | --------------------------------------------- |
| `llm`  | Uses GPT-4o for high-accuracy JSON extraction |
| `rule` | Uses regex-based parsing for offline fallback |

---

## ğŸ“ˆ Future Enhancements

* [ ] JD matching with embedding comparison
* [ ] DB storage (Mongo/Postgres)
* [ ] Async background processing via Celery
* [ ] User-auth + dashboard
* [ ] PDF annotation with extracted data

---
